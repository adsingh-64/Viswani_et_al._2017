# Viswani_et_al._2017
Implementation of the encoder-decoder transformer from Attention is All You Need using the GPT-2 tokenizer. Full training process of WMT-14 data loading, data preprocessing for batched training, model definition, and model training can be run from NMT.py.
