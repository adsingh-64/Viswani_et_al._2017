{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maste\\miniconda3\\envs\\appenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wmt14\", \"fr-en\", split=\"train[:1000000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = 50256\n",
    "pad_token_id = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    french_sentences = [item['fr'] for item in examples['translation']]\n",
    "    english_sentences = [item['en'] for item in examples['translation']]\n",
    "    \n",
    "    # Tokenize with tiktoken without adding <EOS> token\n",
    "    french_tokens = [torch.tensor(tokenizer.encode(sentence)) for sentence in french_sentences]\n",
    "    english_tokens = [torch.tensor(tokenizer.encode(sentence)) for sentence in english_sentences]\n",
    "    \n",
    "    return french_tokens, english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences, english_sentences = tokenize_function(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_context_length = 30\n",
    "decoder_max_context_length = 20 # english tokenization more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(sequences, max_length, pad_token_id):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        seq_length = seq.shape[0]\n",
    "        if seq_length > max_length:\n",
    "            seq = seq[:max_length]\n",
    "        else:\n",
    "            padding_length = max_length - seq_length\n",
    "            padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            seq = torch.cat([seq, padding], dim=0)\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "encoder_inputs = pad_or_truncate(\n",
    "    french_sentences, encoder_max_context_length, pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_inputs = []\n",
    "decoder_labels = []\n",
    "\n",
    "for seq in english_sentences:\n",
    "    seq_length = seq.shape[0]\n",
    "\n",
    "    seq = seq[:decoder_max_context_length - 1]\n",
    "\n",
    "    # Prepare decoder inputs by prepending <EOS>\n",
    "    input_seq = torch.cat([torch.tensor([eos_token_id]), seq], dim=0)\n",
    "\n",
    "    # Prepare decoder labels by appending <EOS>\n",
    "    label_seq = torch.cat([seq, torch.tensor([eos_token_id])], dim=0)\n",
    "\n",
    "    # Pad decoder inputs to decoder_max_context_length if needed\n",
    "    if input_seq.shape[0] < decoder_max_context_length:\n",
    "        padding_length = decoder_max_context_length - input_seq.shape[0]\n",
    "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "        input_seq = torch.cat([input_seq, padding], dim=0)\n",
    "\n",
    "    # Pad decoder labels to decoder_max_context_length if needed\n",
    "    if label_seq.shape[0] < decoder_max_context_length:\n",
    "        padding_length = decoder_max_context_length - label_seq.shape[0]\n",
    "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "        label_seq = torch.cat([label_seq, padding], dim=0)\n",
    "\n",
    "    decoder_inputs.append(input_seq)\n",
    "    decoder_labels.append(label_seq)\n",
    "\n",
    "# Convert lists to tensors\n",
    "decoder_inputs = torch.stack(decoder_inputs)\n",
    "decoder_labels = torch.stack(decoder_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50258\n",
    "encoder_block_size = 30\n",
    "decoder_block_size = 20\n",
    "d_model = 32\n",
    "n_head = 4\n",
    "n_encoder_layers = 3\n",
    "n_decoder_layers = 3\n",
    "dropout = 0.2\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"linear layer followed by non-linearity\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),  # [B, T, n_embd]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),  # transition matrix to prepare for going back into residual pathway via addition\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.attn(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(d_model, 2) # all [B, T, C]\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T, T]\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
    "        full_att = full_att.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(full_att, dim=-1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T, hs]\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(B, T, C) # concat heads -- [B, T, C]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = EncoderMultiHeadAttention()\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.sa(self.ln1(x), mask) # departure from Attention is All You Need -- we apply LN before transformation\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, shared_embedding):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = shared_embedding\n",
    "        self.position_embedding_table = nn.Embedding(encoder_block_size, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Encoder_Block() for _ in range(n_encoder_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) \n",
    "        x = tok_emb + pos_emb  \n",
    "        mask = (idx != pad_token_id).to(idx.device) # [B, T]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.ln_f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(decoder_block_size, decoder_block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.attn(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(d_model, 2)  # all [B, T, C]\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T, T]\n",
    "        left_att = full_att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        attention_scores = F.softmax(left_att, dim = -1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T, hs]\n",
    "        context_vectors = (context_vectors.transpose(1, 2).contiguous().view(B, T, C))  # concat heads -- [B, T, C]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderEncoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kv = nn.Linear(d_model, 2 * d_model)\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, encoder_outputs_mask):\n",
    "        B, T_decoder, C = x.shape\n",
    "        B, T_encoder, C = encoder_outputs.shape\n",
    "        kv = self.kv(encoder_outputs) # [B, T_encoder, 2 * d_model]\n",
    "        k, v = kv.split(d_model, 2) # each [B, T_encoder, d_model]\n",
    "        q = self.q(x) # [B, T_decoder, d_model]\n",
    "        q = q.view(B, T_decoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_decoder, hs]\n",
    "        k = k.view(B, T_encoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_encoder, hs]\n",
    "        v = v.view(B, T_encoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_encoder, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T_decoder, T_encoder]\n",
    "        mask = encoder_outputs_mask.unsqueeze(1).unsqueeze(2)\n",
    "        full_att = full_att.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(full_att, dim=-1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T_decoder, hs]\n",
    "        context_vectors = (context_vectors.transpose(1, 2).contiguous().view(B, T_decoder, C))  # concat heads -- [B, T_decoder, d_model]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa1 = DecoderMultiHeadAttention()\n",
    "        self.sa2 = DecoderEncoderMultiHeadAttention()\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, encoder_outputs_mask):\n",
    "        x = x + self.sa1(self.ln1(x))  # departure from Attention is All You Need -- we apply LN before transformation\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        x = x + self.sa2(self.ln3(x), encoder_outputs, encoder_outputs_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shared_embedding):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = shared_embedding\n",
    "        self.position_embedding_table = nn.Embedding(decoder_block_size, d_model)\n",
    "        self.blocks = nn.ModuleList([Decoder_Block() for _ in range(n_decoder_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size) # need to tie to the input embedding table (transpose of it)\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "    def forward(self, idx, encoder_outputs, encoder_outputs_mask, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (batch_size, block_size, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))  # (block_size, n_embd)\n",
    "        x = tok_emb + pos_emb  #  (batch_size, block_size, n_embd)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, encoder_outputs, encoder_outputs_mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (batch_size, block_size, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # idx and targets are both (B,T) tensor of integers\n",
    "            B, T, _ = logits.shape\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index = 50257)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, d_model) \n",
    "        with torch.no_grad(): \n",
    "            self.shared_embedding.weight /= (d_model ** 0.5)\n",
    "        self.encoder = Encoder(self.shared_embedding)\n",
    "        self.decoder = Decoder(self.shared_embedding)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, targets=None):\n",
    "        encoder_outputs_mask = (encoder_inputs != pad_token_id).to(device)\n",
    "        encoder_outputs = self.encoder(encoder_inputs)\n",
    "        logits, loss = self.decoder(decoder_inputs, encoder_outputs, encoder_outputs_mask, targets)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(encoder_inputs.shape[0], (batch_size,))\n",
    "    enc_inputs = encoder_inputs[ix] # [batch_size, encoder_context_length]\n",
    "    dec_inputs = decoder_inputs[ix] # [B, decoder_context_length]\n",
    "    dec_labels = decoder_labels[ix]  # [B, decoder_context_length]\n",
    "    return enc_inputs, dec_inputs, dec_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = encoder_inputs.to(device)\n",
    "decoder_inputs = decoder_inputs.to(device)\n",
    "decoder_labels = decoder_labels.to(device)\n",
    "\n",
    "transformer = Transformer().to(device)\n",
    "transformer = torch.compile(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=6e-4)\n",
    "max_iters = 50000\n",
    "eval_iter = 2000\n",
    "\n",
    "num_batches = dataset.num_rows // batch_size\n",
    "num_epochs = max_iters // num_batches\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    enc_inputs, dec_inputs, dec_labels = get_batch()\n",
    "    logits, loss = transformer(enc_inputs, dec_inputs, dec_labels)\n",
    "    if iter % eval_iter == 0:\n",
    "        print(f\"Step {iter} | Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    def __init__(self, tokens, log_prob, length):\n",
    "        self.tokens = tokens\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length\n",
    "\n",
    "    def get_score(self, alpha=0.6):\n",
    "        return self.log_prob / (self.length ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:  Ryder Loyal Slaughter responsibilityuel Cargoisco Kane dysfunctional remindLD throughput throughputvc stimulated Gabriel abusesercise Louisiana\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Bonjour tout le monde!\"\n",
    "encoder_inputs = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0).to(device)\n",
    "encoder_outputs_mask = (encoder_inputs != pad_token_id).to(device)\n",
    "encoder_outputs = transformer.encoder(encoder_inputs)\n",
    "\n",
    "k = 4 \n",
    "\n",
    "initial_hypothesis = Hypothesis([eos_token_id], 0.0, 1)\n",
    "beam = [initial_hypothesis]\n",
    "completed_hypotheses = []\n",
    "\n",
    "for _ in range(decoder_block_size - 1):# predefined cutoff, when it runs the decoder_block_sizeth time, the decoder input will be decoder_block_size length\n",
    "    new_beam = []\n",
    "    for hypothesis in beam:\n",
    "        decoder_input = torch.tensor([hypothesis.tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _ = transformer.decoder(decoder_input, encoder_outputs, encoder_outputs_mask, targets=None)\n",
    "\n",
    "        next_token_logits = logits[:, -1, :].view(-1)\n",
    "        next_token_log_probs = F.log_softmax(next_token_logits, dim = -1)\n",
    "\n",
    "\n",
    "        top_k_log_probs, top_k_indices = torch.topk(next_token_log_probs, k) # both [k]\n",
    "        next_tokens = top_k_indices.tolist()\n",
    "        next_token_log_probs = top_k_log_probs.tolist()\n",
    "\n",
    "        for token, log_prob in zip(next_tokens, next_token_log_probs):\n",
    "            new_tokens = hypothesis.tokens + [token]\n",
    "            new_log_prob = hypothesis.log_prob + log_prob\n",
    "            new_length = hypothesis.length + 1\n",
    "            new_hypothesis = Hypothesis(new_tokens, new_log_prob, new_length)\n",
    "\n",
    "            if token == eos_token_id or len(new_tokens) > decoder_max_context_length:\n",
    "                completed_hypotheses.append(new_hypothesis)\n",
    "            else:\n",
    "                new_beam.append(new_hypothesis)\n",
    "\n",
    "    if not new_beam: # stop early if all hypotheses have been completed\n",
    "        break\n",
    "\n",
    "    beam = sorted(new_beam, key = lambda h: h.get_score(), reverse=True)[:k] # keep top k\n",
    "\n",
    "if completed_hypotheses:\n",
    "    best_hypothesis = max(completed_hypotheses, key=lambda h: h.get_score())\n",
    "else: # if not completed hypotheses, then beam must be non-empty so if-else partitions event space\n",
    "    best_hypothesis = max(beam, key=lambda h: h.get_score())\n",
    "\n",
    "output_tokens = best_hypothesis.tokens[1:]\n",
    "\n",
    "translation = tokenizer.decode(output_tokens, errors=\"replace\")\n",
    "\n",
    "print(f\"Translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
