{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maste\\miniconda3\\envs\\appenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wmt14\", \"fr-en\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = 50256\n",
    "pad_token_id = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    french_sentences = [item['fr'] for item in examples['translation']]\n",
    "    english_sentences = [item['en'] for item in examples['translation']]\n",
    "    \n",
    "    # Tokenize with tiktoken without adding <EOS> token\n",
    "    french_tokens = [torch.tensor(tokenizer.encode(sentence)) for sentence in french_sentences]\n",
    "    english_tokens = [torch.tensor(tokenizer.encode(sentence)) for sentence in english_sentences]\n",
    "    \n",
    "    return french_tokens, english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences, english_sentences = tokenize_function(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_context_length = 30\n",
    "decoder_max_context_length = 20 # english tokenization more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(sequences, max_length, pad_token_id):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        seq_length = seq.shape[0]\n",
    "        if seq_length > max_length:\n",
    "            seq = seq[:max_length]\n",
    "        else:\n",
    "            padding_length = max_length - seq_length\n",
    "            padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "            seq = torch.cat([seq, padding], dim=0)\n",
    "        padded_sequences.append(seq)\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "encoder_inputs = pad_or_truncate(\n",
    "    french_sentences, encoder_max_context_length, pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_inputs = []\n",
    "decoder_labels = []\n",
    "\n",
    "for seq in english_sentences:\n",
    "    seq_length = seq.shape[0]\n",
    "\n",
    "    seq = seq[:decoder_max_context_length - 1]\n",
    "\n",
    "    # Prepare decoder inputs by prepending <EOS>\n",
    "    input_seq = torch.cat([torch.tensor([eos_token_id]), seq], dim=0)\n",
    "\n",
    "    # Prepare decoder labels by appending <EOS>\n",
    "    label_seq = torch.cat([seq, torch.tensor([eos_token_id])], dim=0)\n",
    "\n",
    "    # Pad decoder inputs to decoder_max_context_length if needed\n",
    "    if input_seq.shape[0] < decoder_max_context_length:\n",
    "        padding_length = decoder_max_context_length - input_seq.shape[0]\n",
    "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "        input_seq = torch.cat([input_seq, padding], dim=0)\n",
    "\n",
    "    # Pad decoder labels to decoder_max_context_length if needed\n",
    "    if label_seq.shape[0] < decoder_max_context_length:\n",
    "        padding_length = decoder_max_context_length - label_seq.shape[0]\n",
    "        padding = torch.full((padding_length,), pad_token_id, dtype=torch.long)\n",
    "        label_seq = torch.cat([label_seq, padding], dim=0)\n",
    "\n",
    "    decoder_inputs.append(input_seq)\n",
    "    decoder_labels.append(label_seq)\n",
    "\n",
    "# Convert lists to tensors\n",
    "decoder_inputs = torch.stack(decoder_inputs)\n",
    "decoder_labels = torch.stack(decoder_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50258\n",
    "encoder_block_size = 30\n",
    "decoder_block_size = 20\n",
    "d_model = 32\n",
    "n_head = 4\n",
    "n_encoder_layers = 3\n",
    "n_decoder_layers = 3\n",
    "dropout = 0.2\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"linear layer followed by non-linearity\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),  # [B, T, n_embd]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),  # transition matrix to prepare for going back into residual pathway via addition\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.attn(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(d_model, 2) # all [B, T, C]\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T, T]\n",
    "        mask = mask.unsqueeze(1).unsqueeze(2) # [B, 1, 1, T]\n",
    "        full_att = full_att.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(full_att, dim=-1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T, hs]\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous().view(B, T, C) # concat heads -- [B, T, C]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa = EncoderMultiHeadAttention()\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.sa(self.ln1(x), mask) # departure from Attention is All You Need -- we apply LN before transformation\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, shared_embedding):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = shared_embedding\n",
    "        self.position_embedding_table = nn.Embedding(encoder_block_size, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Encoder_Block() for _ in range(n_encoder_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) \n",
    "        x = tok_emb + pos_emb  \n",
    "        mask = (idx != pad_token_id).to(idx.device) # [B, T]\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        x = self.ln_f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(decoder_block_size, decoder_block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.attn(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(d_model, 2)  # all [B, T, C]\n",
    "        q = q.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        k = k.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        v = v.view(B, T, n_head, C // n_head).transpose(1, 2)  # [B, nh, T, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T, T]\n",
    "        left_att = full_att.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        attention_scores = F.softmax(left_att, dim = -1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T, hs]\n",
    "        context_vectors = (context_vectors.transpose(1, 2).contiguous().view(B, T, C))  # concat heads -- [B, T, C]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderEncoderMultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kv = nn.Linear(d_model, 2 * d_model)\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, encoder_outputs_mask):\n",
    "        B, T_decoder, C = x.shape\n",
    "        B, T_encoder, C = encoder_outputs.shape\n",
    "        kv = self.kv(encoder_outputs) # [B, T_encoder, 2 * d_model]\n",
    "        k, v = kv.split(d_model, 2) # each [B, T_encoder, d_model]\n",
    "        q = self.q(x) # [B, T_decoder, d_model]\n",
    "        q = q.view(B, T_decoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_decoder, hs]\n",
    "        k = k.view(B, T_encoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_encoder, hs]\n",
    "        v = v.view(B, T_encoder, n_head, C // n_head).transpose(1, 2)  # [B, nh, T_encoder, hs]\n",
    "        full_att = q @ k.transpose(-2, -1) / (k.shape[-1]) ** 0.5  # [B, nh, T_decoder, T_encoder]\n",
    "        mask = encoder_outputs_mask.unsqueeze(1).unsqueeze(2)\n",
    "        full_att = full_att.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(full_att, dim=-1)\n",
    "        context_vectors = attention_scores @ v  # [B, nh, T_decoder, hs]\n",
    "        context_vectors = (context_vectors.transpose(1, 2).contiguous().view(B, T_decoder, C))  # concat heads -- [B, T_decoder, d_model]\n",
    "        out = self.dropout(self.proj(context_vectors))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sa1 = DecoderMultiHeadAttention()\n",
    "        self.sa2 = DecoderEncoderMultiHeadAttention()\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, encoder_outputs_mask):\n",
    "        x = x + self.sa1(self.ln1(x))  # departure from Attention is All You Need -- we apply LN before transformation\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        x = x + self.sa2(self.ln3(x), encoder_outputs, encoder_outputs_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, shared_embedding):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = shared_embedding\n",
    "        self.position_embedding_table = nn.Embedding(decoder_block_size, d_model)\n",
    "        self.blocks = nn.ModuleList([Decoder_Block() for _ in range(n_decoder_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size) # need to tie to the input embedding table (transpose of it)\n",
    "        self.lm_head.weight = self.token_embedding_table.weight\n",
    "\n",
    "    def forward(self, idx, encoder_outputs, encoder_outputs_mask, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)  # (batch_size, block_size, n_embd)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))  # (block_size, n_embd)\n",
    "        x = tok_emb + pos_emb  #  (batch_size, block_size, n_embd)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, encoder_outputs, encoder_outputs_mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (batch_size, block_size, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # idx and targets are both (B,T) tensor of integers\n",
    "            B, T, _ = logits.shape\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(-1)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index = 50257)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, d_model) \n",
    "        with torch.no_grad(): \n",
    "            self.shared_embedding.weight /= (d_model ** 0.5)\n",
    "        self.encoder = Encoder(self.shared_embedding)\n",
    "        self.decoder = Decoder(self.shared_embedding)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, targets=None):\n",
    "        encoder_outputs_mask = (encoder_inputs != pad_token_id).to(device)\n",
    "        encoder_outputs = self.encoder(encoder_inputs)\n",
    "        logits, loss = self.decoder(decoder_inputs, encoder_outputs, encoder_outputs_mask, targets)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(encoder_inputs.shape[0], (batch_size,))\n",
    "    enc_inputs = encoder_inputs[ix] # [batch_size, encoder_context_length]\n",
    "    dec_inputs = decoder_inputs[ix] # [B, decoder_context_length]\n",
    "    dec_labels = decoder_labels[ix]  # [B, decoder_context_length]\n",
    "    return enc_inputs, dec_inputs, dec_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = encoder_inputs.to(device)\n",
    "decoder_inputs = decoder_inputs.to(device)\n",
    "decoder_labels = decoder_labels.to(device)\n",
    "\n",
    "transformer = Transformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.368083000183105\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=6e-4)\n",
    "max_iters = 5000\n",
    "eval_iter = 500\n",
    "\n",
    "for iter in max_iters:\n",
    "    enc_inputs, dec_inputs, dec_labels = get_batch()\n",
    "    logits, loss = transformer(enc_inputs, dec_inputs, dec_labels)\n",
    "    if iter % eval_iter == 0:\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypothesis:\n",
    "    def __init__(self, tokens, log_prob, length):\n",
    "        self.tokens = tokens\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length\n",
    "\n",
    "    def get_score(self):\n",
    "        return self.log_prob / self.length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Bonjour tout le monde!\"\n",
    "encoder_inputs = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0).to(device)\n",
    "encoder_outputs_mask = (encoder_inputs != pad_token_id).to(device)\n",
    "encoder_outputs = transformer.encoder(encoder_inputs)\n",
    "\n",
    "k = 2\n",
    "max_completed_hypotheses = 1\n",
    "\n",
    "initial_hypothesis = Hypothesis([eos_token_id], 0.0, 1)\n",
    "beam = [initial_hypothesis]\n",
    "completed_hypotheses = []\n",
    "\n",
    "while len(completed_hypotheses) < max_completed_hypotheses:\n",
    "    new_beam = []\n",
    "    for hypothesis in beam:\n",
    "        decoder_input = torch.tensor([hypothesis.tokens], dtype=torch.long, device=device)\n",
    "        logits, _ = transformer.decoder(decoder_input, encoder_outputs, encoder_outputs_mask, targets=None)\n",
    "        next_token_logits = logits[:, -1, :].view(-1)\n",
    "        next_token_probs = F.softmax(next_token_logits, dim = -1)\n",
    "        _, top_k_indices = torch.topk(next_token_logits, k) # [k]\n",
    "        next_tokens = [index.item() for index in top_k_indices]\n",
    "        next_token_log_probs = [torch.log(next_token_probs[token]).item() for token in next_tokens]\n",
    "        new_hypotheses = [Hypothesis(hypothesis.tokens + [token], hypothesis.log_prob + log_prob, hypothesis.length + 1) \n",
    "                          for token, log_prob in zip(next_tokens, next_token_log_probs)]\n",
    "        for new_hypothesis in new_hypotheses:\n",
    "            if new_hypothesis.tokens[-1] == 50256 or len(new_hypothesis.tokens) > decoder_max_context_length:\n",
    "                completed_hypotheses.append(new_hypothesis)\n",
    "        new_beam += new_hypotheses\n",
    "    beam = new_beam\n",
    "\n",
    "# can just use logits\n",
    "# normalize by scores\n",
    "# keep top k overall, not just for each current hypothesis -- explore more candidates each turn?    \n",
    "# infinite loop if beam becomes empty\n",
    "# use lambda functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
